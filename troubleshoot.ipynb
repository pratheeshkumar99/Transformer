{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3pas6eiLp_kD","executionInfo":{"status":"ok","timestamp":1721394071658,"user_tz":-330,"elapsed":19409,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/My Drive/MyProjects/Transformers"],"metadata":{"id":"_WLi7FUordVh","outputId":"e4febb38-f638-4f4f-b05a-3f659470bc4e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721394079631,"user_tz":-330,"elapsed":509,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MyProjects/Transformers\n"]}]},{"cell_type":"code","source":["ls \".\""],"metadata":{"id":"K4xT5RsCrgJI","outputId":"c782d6c0-a5e7-4ae5-e153-ec81c9732c50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721394080466,"user_tz":-330,"elapsed":440,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["attention_visual.ipynb  config.py          model.py           requirements.txt   train_.py\n","Beam_Search.ipynb       dataset.py         model_train.ipynb  \u001b[0m\u001b[01;34mruns\u001b[0m/              train_wb.py\n","Colab_Train.ipynb       Inference.ipynb    \u001b[01;34m__pycache__\u001b[0m/       tokenizer_en.json  translate.py\n","conda.txt               Local_Train.ipynb  README.md          tokenizer_it.json  troubleshoot.ipynb\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install datasets\n","!pip install tokenizers\n","!pip install torchmetrics"],"metadata":{"id":"zle2SzOSsVdW","executionInfo":{"status":"ok","timestamp":1721394196487,"user_tz":-330,"elapsed":116022,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0A6xQpyhqUGz","executionInfo":{"status":"ok","timestamp":1721394196487,"user_tz":-330,"elapsed":5,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"outputs":[],"source":["# import shutil\n","\n","# def delete_directory(path):\n","#     try:\n","#         shutil.rmtree(path)\n","#         print(f\"The directory at {path} has been deleted successfully.\")\n","#     except Exception as e:\n","#         print(f\"Failed to delete the directory: {e}\")\n","\n","# # Usage\n","# delete_directory('/content/.config')"]},{"cell_type":"code","source":["import torch\n","! pip install matplotlib\n","from train_ import run_validation,get_model,Tokenizer,build_transformer,get_ds\n","from config import get_config\n","from pathlib import Path\n","from torch.utils.data import DataLoader,Dataset\n","from datasets import load_dataset\n","from torch.utils.data import random_split\n","from dataset import BilingualDataset\n","from datasets import load_dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IILD00acq0t1","outputId":"d1233f25-f02a-4792-c9b2-031d09a1224a","executionInfo":{"status":"ok","timestamp":1721394227472,"user_tz":-330,"elapsed":30989,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"code","source":["config = get_config()\n","print(config)"],"metadata":{"id":"w_ED1mVptbRi","outputId":"ed3507c0-2db7-4146-812b-5ff7bb26e7ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721394227472,"user_tz":-330,"elapsed":5,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{'batch_size': 8, 'num_epochs': 20, 'lr': 0.0001, 'seq_len': 350, 'd_model': 512, 'datasource': 'opus_books', 'lang_src': 'en', 'lang_tgt': 'it', 'model_folder': '/content/drive/MyDrive/Models/pytorch-transformer/weights', 'model_basename': 'tmodel_', 'preload': 'latest', 'tokenizer_file': 'tokenizer_{0}.json', 'experiment_name': 'runs/tmodel'}\n"]}]},{"cell_type":"code","source":["source_tokenizer_path = str(Path( \"/content/drive/My Drive/Models/pytorch-transformer/vocab/tokenizer_en.json\"))\n","dest_tokenizer_path = str(Path( \"/content/drive/My Drive/Models/pytorch-transformer/vocab/tokenizer_it.json\"))\n","print(source_tokenizer_path,dest_tokenizer_path)"],"metadata":{"id":"FeDx57GzrkJ7","outputId":"eaa93eb5-eab5-4609-bb8a-2be89bbe6724","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721394227472,"user_tz":-330,"elapsed":3,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Models/pytorch-transformer/vocab/tokenizer_en.json /content/drive/My Drive/Models/pytorch-transformer/vocab/tokenizer_it.json\n"]}]},{"cell_type":"code","source":["tokenizer_src = Tokenizer.from_file(source_tokenizer_path)\n","tokenizer_tgt = Tokenizer.from_file(dest_tokenizer_path)"],"metadata":{"id":"PEfA-BWHrUNa","executionInfo":{"status":"ok","timestamp":1721394228580,"user_tz":-330,"elapsed":435,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["device =  \"cpu\"\n","model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n","model.to(device)"],"metadata":{"id":"Lf71U5rit1OZ","outputId":"a8b9e79c-51c2-4318-f3c2-3ff0190b4c26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721394232688,"user_tz":-330,"elapsed":4110,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Transformer(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x EncoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=512, out_features=512, bias=False)\n","          (w_k): Linear(in_features=512, out_features=512, bias=False)\n","          (w_v): Linear(in_features=512, out_features=512, bias=False)\n","          (w_o): Linear(in_features=512, out_features=512, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-1): 2 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x DecoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=512, out_features=512, bias=False)\n","          (w_k): Linear(in_features=512, out_features=512, bias=False)\n","          (w_v): Linear(in_features=512, out_features=512, bias=False)\n","          (w_o): Linear(in_features=512, out_features=512, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (cross_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=512, out_features=512, bias=False)\n","          (w_k): Linear(in_features=512, out_features=512, bias=False)\n","          (w_v): Linear(in_features=512, out_features=512, bias=False)\n","          (w_o): Linear(in_features=512, out_features=512, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-2): 3 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (src_embed): InputEmbeddings(\n","    (embedding): Embedding(15698, 512)\n","  )\n","  (tgt_embed): InputEmbeddings(\n","    (embedding): Embedding(22463, 512)\n","  )\n","  (src_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (tgt_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (projection_layer): ProjectionLayer(\n","    (proj): Linear(in_features=512, out_features=22463, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["model_filename = \"/content/drive/My Drive/Models/pytorch-transformer/tmodel_3.pt\"\n","state = torch.load(model_filename)\n","model.load_state_dict(state['model_state_dict'])"],"metadata":{"id":"e133USX6t38W","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1721394234360,"user_tz":-330,"elapsed":1675,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}},"outputId":"4af12d5d-c00b-45ac-bc53-893714d7bd49"},"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-19704e0d207f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Models/pytorch-transformer/tmodel_3.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             _internal=True)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    250\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."]}]},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"irO4e8wt0FN5","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def get_data(config,tokenizer_src,tokenizer_tgt):\n","    # It only has the train split, so we divide it overselves\n","    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n","\n","    # Build tokenizers\n","    tokenizer_src = tokenizer_src\n","    tokenizer_tgt = tokenizer_tgt\n","\n","    # Keep 90% for training, 10% for validation\n","    train_ds_size = int(0.9 * len(ds_raw))\n","    val_ds_size = len(ds_raw) - train_ds_size\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n","\n","    # Find the maximum length of each sentence in the source and target sentence\n","    max_len_src = 0\n","    max_len_tgt = 0\n","\n","    for item in ds_raw:\n","        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n","        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n","        max_len_src = max(max_len_src, len(src_ids))\n","        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n","\n","    print(f'Max length of source sentence: {max_len_src}')\n","    print(f'Max length of target sentence: {max_len_tgt}')\n","\n","\n","    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n","    val_dataloader = DataLoader(val_ds, batch_size=8, shuffle=True)\n","\n","    return train_dataloader,val_dataloader\n"],"metadata":{"id":"gRWT_hhI0NkJ","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data,val_dataloader = get_data(config,tokenizer_src,tokenizer_tgt)"],"metadata":{"id":"nyThEDAY0XRs","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchmetrics\n","import os\n","import matplotlib.pyplot as plt\n","from train_ import greedy_batch_decode\n","from dataset import causal_mask\n","import torch.nn.functional as F"],"metadata":{"id":"n___TYTV2Otc","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cer = []\n","wer = []\n","bleu = []\n","val_losses = []\n","\n","\n","\n","# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n","#     sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","#     eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","#     # Precompute the encoder output and reuse it for every step\n","#     encoder_output = model.encode(source, source_mask)\n","#     # Initialize the decoder input with the sos token\n","#     decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n","#     while True:\n","#         if decoder_input.size(1) == max_len:\n","#             break\n","\n","#         # build mask for target\n","#         decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n","\n","#         # calculate output\n","#         out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n","\n","#         # get next token\n","#         prob = model.project(out[:, -1])\n","#         _, next_word = torch.max(prob, dim=1)\n","#         decoder_input = torch.cat(\n","#             [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n","#         )\n","\n","#         if next_word == eos_idx:\n","#             break\n","\n","#     result = decoder_input.squeeze(0)\n","#     return result\n","\n","\n","\n","\n","def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device, return_logits=False):\n","    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","    # Precompute the encoder output and reuse it for every step\n","    encoder_output = model.encode(source, source_mask)\n","    # Initialize the decoder input with the sos token\n","    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n","\n","    # Optionally collect logits for loss computation\n","    logits_list = []\n","\n","    while True:\n","        if decoder_input.size(1) >= max_len:\n","            break\n","\n","        # build mask for target\n","        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n","\n","        # calculate output\n","        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n","\n","        # project to vocabulary\n","        prob = model.project(out[:, -1])  # Ensure this outputs logits\n","        _, next_word = torch.max(prob, dim=1)\n","        decoder_input = torch.cat(\n","            [decoder_input, next_word.unsqueeze(0).unsqueeze(0)], dim=1\n","        )\n","\n","        # Collect logits if needed for loss calculation\n","        if return_logits:\n","            logits_list.append(prob.unsqueeze(1))\n","\n","        if next_word.item() == eos_idx:\n","            break\n","\n","    if return_logits:\n","        # Concatenate along sequence length dimension\n","        logits = torch.cat(logits_list, dim=1)\n","        return logits\n","    else:\n","        return decoder_input.squeeze(0)\n","\n","\n","\n","# def greedy_batch_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n","#     # Decode each input sequence in the batch individually\n","#     results = [greedy_decode(model, src.unsqueeze(0), mask.unsqueeze(0), tokenizer_src, tokenizer_tgt, max_len, device) for src, mask in zip(source, source_mask)]\n","\n","#     # Define a fixed maximum length (350) for padding\n","#     fixed_max_length = max_len\n","\n","#     # Pad each decoded sequence to the fixed maximum length with the PAD token\n","#     padded_results = [F.pad(result, (0, fixed_max_length - result.size(0)), 'constant', tokenizer_tgt.token_to_id('[PAD]')) for result in results]\n","\n","#     # Stack the padded sequences into a single tensor for batch processing\n","#     results_tensor = torch.stack(padded_results)\n","#     print(\"Shape after padding to fixed max length:\", results_tensor.shape)\n","#     return results_tensor\n","\n","\n","def greedy_batch_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n","    # Decode each input sequence in the batch individually and collect logits\n","    results = [greedy_decode(model, src.unsqueeze(0), mask.unsqueeze(0), tokenizer_src, tokenizer_tgt, max_len, device, return_logits=True) for src, mask in zip(source, source_mask)]\n","\n","    # Determine the maximum sequence length from the logits for uniform padding\n","    max_length = max(result.size(1) for result in results)\n","\n","    # Pad each decoded logits tensor to the maximum sequence length\n","    padded_results = [F.pad(result, (0, 0, 0, max_length - result.size(1)), 'constant', tokenizer_tgt.token_to_id('[PAD]')) for result in results]\n","\n","    # Stack the padded logits tensors into a single tensor for batch processing\n","    # results_tensor = torch.stack(padded_results)\n","    # print(\"Shape after padding to fixed max length:\", results_tensor.shape)\n","    # return results_tensor\n","    for data in padded_results:\n","        print(data.shape)\n","    return\n","    return torch.stack(padded_results)\n","\n","# This function will now correctly return a tensor of shape [batch_size, seq_len, vocab_size]\n","# where seq_len is the maximum sequence length in the batch, and vocab_size is the number of tokens.\n","\n","def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device):\n","\n","    model.eval()\n","    total_loss = 0\n","    metric_cer = torchmetrics.CharErrorRate()\n","    metric_wer = torchmetrics.WordErrorRate()\n","    metric_bleu = torchmetrics.BLEUScore()\n","\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","    source_texts, expected, predicted = [], [], []\n","\n","\n","    try:\n","        with os.popen('stty size', 'r') as console:\n","            _, console_width = console.read().split()\n","            console_width = int(console_width)\n","    except:\n","        console_width = 80\n","\n","    with torch.no_grad():\n","        for batch in validation_ds:\n","            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n","            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n","            target = batch[\"label\"].to(device) # (b, seq_len)\n","\n","\n","\n","            model_out = greedy_batch_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device) # (b, seq_len)\n","            # loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","            # lloss = loss_fn(model_out.view(-1, tokenizer_tgt.get_vocab_size()), target.view(-1))\n","            print(\"Model output shape:\", model_out.shape)\n","            print(\"Model traget shape:\",target.shape)\n","            print(\"Reshaped model output for loss calculation:\", model_out.view(-1, tokenizer_tgt.get_vocab_size()).shape)\n","            loss = loss_fn(model_out.view(-1, tokenizer_tgt.get_vocab_size()), target.view(-1))\n","            total_loss += loss.item() * encoder_input.size(0)  # Multiply by batch size for accurate average\n","\n","\n","\n","            for idx in range(encoder_input.size(0)):\n","                source_text = tokenizer_src.decode(encoder_input[idx].cpu().numpy())\n","                target_text = tokenizer_tgt.decode(target[idx].cpu().numpy())\n","                predicted_text = tokenizer_tgt.decode(model_out[idx].cpu().numpy())\n","                print(f\"Source: {source_text}\")\n","                print(f\"Target: {target_text}\")\n","                print(f\"Predicted: {predicted_text}\")\n","                print(\"-\" * console_width)\n","\n","                source_texts.append(source_text)\n","                expected.append(target_text)\n","                predicted.append(predicted_text)\n","\n","\n","\n","\n","    # Calculate and log metrics\n","    avg_loss = total_loss / len(validation_ds.dataset)\n","    val_losses.append(avg_loss)\n","    cer_data = metric_cer(predicted, expected)\n","    wer_data = metric_wer(predicted, expected)\n","    bleu_data = metric_bleu(predicted, expected)\n","    cer.append(cer_data)\n","    wer.append(wer_data)\n","    bleu.append(bleu_data)\n","\n","\n","    print(f'Validation Loss: {avg_loss:.4f}, CER: {cer_data:.4f}, WER: {wer_data:.4f}, BLEU: {bleu_data:.4f}')\n","    return avg_loss, cer, wer, bleu\n","\n","\n"],"metadata":{"id":"pcbx3ifE1tfr","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["average_loss,cer,wer,bleu = run_validation(model,val_dataloader,tokenizer_src,tokenizer_tgt,config[\"seq_len\"],device)"],"metadata":{"id":"nItMaEOA0wNM","executionInfo":{"status":"aborted","timestamp":1721394234361,"user_tz":-330,"elapsed":2,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for batch in val_dataloader:\n","    print(batch[\"encoder_input\"].shape)\n","    print(batch[\"encoder_mask\"].shape)\n","    print(batch[\"label\"].shape)\n","    break"],"metadata":{"id":"n7xJ2mFl0yW7","executionInfo":{"status":"aborted","timestamp":1721392299783,"user_tz":-330,"elapsed":4,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2P4RPEn-0Bi","executionInfo":{"status":"ok","timestamp":1721392387465,"user_tz":-330,"elapsed":951,"user":{"displayName":"pratheesh pratheesh","userId":"01451680363797990753"}},"outputId":"b3dcff7a-d75e-4719-ba2e-1e6a184d528c"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["350"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":[],"metadata":{"id":"E3pkTeRj-04U"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}